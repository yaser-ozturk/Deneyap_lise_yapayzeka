
## Big Data

Günümüzde teknolojinin hızla gelişmesiyle bilgisayar, cep telefonu, tablet gibi akıllı cihazların ve internet teknolojisinin hızla yayılması ile üretilen veriler artmış ve “büyük veri” kavramı ortaya çıkmıştır. 

Tanım olarak Big Data, verinin analiz edilip sınıflandırılmış, anlamlı ve işlenebilir hale dönüştürülmüş halidir.

Büyük veri; verinin hacmi, veri hızı, veri çeşitliliği, verinin düzensiz veya yanlış olması, verinin değerli olması olmak üzere beş bileşenden oluşur.

![[Pasted image 20260102015751.png]]

### Veri hacmi

Verinin yüksek hacimli olduğunu belirtir. 

Örneğin bir uçağın motorunda ve diğer bileşenlerinde milyonlarca sensör mevcuttur. Bu sensörler uçakların yaptığı her bir hareketi anlık olarak toplayarak terabaytlar seviyesinde veri üretmektedir.

![[Pasted image 20260102014143.png]]

![[Pasted image 20260102014321.png]]

### Velocity(Veri Hızı):


![](https://miro.medium.com/v2/resize:fit:1050/1*0DFC7M2zFTGFRJNh8x2PTg.jpeg)

Adı üzerinde olduğu gibi Velocity, Veri Hızı anlamına gelmektedir. Bunun anlamı ise saniyeler veya dakikalar içerisinde size ne kadar veri ulaştığıdır. Özellikle ve özellikle herkesin bildiği gibi Sosyal Medya’da binlerce işlem yapılıyor. Burada gördüğünüz bir Sosyal Medya infografiği.

Örneğin;

· Google’da dakikada 4 milyon arama.
· Youtube’da ki kullanıcılar dakikada 72 saatlik video yüklüyormuş.
· Facebook’da dakikada 2 milyon 460 bin içerik paylaşılıyormuş.
· Amazon dakikada 83 bin dolarlık online satış yapıyormuş.
· Twitter’da dakikada 277 bin tweet atılıyormuş.

Infograf’ta da gördüğünüz gibi inanılmaz bir veri akışı yalnızca dakikalar içerisinde oluşuyor ve işlenmek için size ulaşıyor.

### Variety(Veri Çeşitliliği):


![](https://miro.medium.com/v2/resize:fit:1050/1*c8BUrwtAaOodQYraDXaNkw.jpeg)

Big Data’da verilerin belirli bir yapısı veya formatı yoktur. Json, txt, fotoğraf, video veya Sosyal Medya gibi verilerin çeşitliliğini düşünürsek elimizde büyük bir veri yığını oluşmuş olur. Big Data bunların hepsini aynı formatta toplayıp hepsini işleyebilir.

###  Verification (Gerçeklik):


![](https://miro.medium.com/v2/resize:fit:1050/1*XtY4wLCLHWkA2ZzSEU7HiA.jpeg)

Big Data içerisinde çok büyük veri yığınları bulunduğundan içerisinde anlamsız verilerin olması kadar normal bir durum olamaz. Veri Madenciliği’nde bu anlamsız verilere “Kirli yada Gürültücü Veri” deniyor. Bu anlamsız kayıtların sağlıklı bir sonuç alınabilmesi için temizlenmesi gerekmektedir. 

Örneğin trafikteki araçların süratlerini değerlendirirken bir araçtan eksi sonuç geliyorsa bu durum yanlıştır. Çünkü fizikte sürat her zaman pozitif bir değerdir. Bu sebepten bu değerlerin temizlenmesi gerekmektedir.

![[Pasted image 20260102020924.png]]

### Value(Değer- Anlamlandırma):


![](https://miro.medium.com/v2/resize:fit:1050/1*TE_Tsw94Z2sbKpIRyT_X6A.jpeg)

Big Data’nın en önemli birleşenlerinden biriside Value yani katma değer yaratmasıdır. Bunu şirketlerin yaptığı çalışlamarla örneklendirirsek çok daha iyi anlaşılacaktır.

Netflix:


![](https://miro.medium.com/v2/resize:fit:1050/1*nhAV-exVRsyu3Saqv6sOFA.jpeg)

Netfilix aslında 99–2000 yıllarında kurulmuştur ama parlama zamanı 2008–2009 yıllarında olmuştuır. Yıldızının parlamasına yol açan en büyük teknolojilerden bir taneside Big Data teknolojisidir. Peki bunu nasıl başarabildi!? Bildiğiniz gibi bizde dahil olmak üzere çoğu web sitesi kullanıcılarının loglarını tutmaktadır. Ancak Netflix bunu birkaç adım ileri taşımıştır. Kullanıcının videoları izleme sürelerini, kaç kere durduklarını, ses seviyenizi hatta ve hatta izlediğiniz filmlerdeki oyuncuların sayfalarını beğenip beğenmediğinizi, tweet atıp atmadığınıza kadar birçok bilgiyi tutmaktadır ve sizinle alakalı bir model oluşturmaktadır. Dünyanın herhangi bir yerinnde sizinle aynı yada çok benzer bir modele sahip kullanıcıyı eşleştirir.

Örneğin çok sıkı bir Christoper Nolan hayranısınız, bilim Kurgu izlemeye bayılıyorsunuz ve tüm filmleri durdurmadan son ses izliyorsunuz. Bu eşleşme sayesinde eşleştiğiniz kişi başka bir film izlediğinde sizinde bu filmi beğenme ihtimaliniz çok yüksek diye sizede bu filmi öneriyor. Özetle, Netflix bizi bizden daha iyi tanıyor. Bunun altında ise Big Data’nın gücü yatıyor.

## Makine Öğrenmesi Temel Anlatım

Büyük veriyi değerlendirmek için makine öğrenme yöntemleri kullanılır.

Makine öğrenmesi, bilgisayarın meydana gelen bir olay ile ilgili topladığı bilgi ve tecrübeleri öğrenebilmesi amacıyla matematiksel modellerin kullanılmasıdır. Makine öğrenmesinde temel amaç elde edilen veriler ile gelecekte oluşabilecek benzer olaylar hakkında kararlar verebilmek veya geçmişteki durumlar hakkında sonuç oluşturmaktır.

İZLEYELİM :

https://www.youtube.com/watch?v=-O-E1nFm6-A

![[Pasted image 20260102022228.png]]

Makine öğrenmesi sürecini temel olarak dört aşamadan oluşmaktadır. İlk aşamada, akıllı cihazlardan veriler toplanarak işlenir. İşleme sürecinde uygun olmayan veriler veri setinden çıkarılarak veri bütünlüğü sağlanır. İkinci aşamada, veri setindeki veriler bir kısmı eğitim verisi diğer kısmı test verisi olarak ikiye ayrılır. Eğitim verileri makine öğrenmesindeki modelleri eğitmek için kullanılır. Üçüncü aşamada, modellerden elde edilen sonuçlar test verileri ile analiz edilerek makine öğrenmesi modelinin doğruluğu test edilir. Son aşamada ise, test verilerinden elde edilen sonuçlar değerlendirilir.

## Makine Öğrenmesi Teknikleri

Makine öğrenmesi tekniklerinde sıklıkla naive-bayes algoritmaları, destek vektör makineleri, karar ağacı algoritmaları, k-en yakın komşu algoritmaları kullanılmaktadır.

![[Pasted image 20260102022414.png]]

Makine öğrenmesinde, modellerin eğitilme şekillerine göre temel olarak **Denetimli Öğrenme** (Supervised Learning) ve **Denetimsiz Öğrenme** (Unsupervised Learning) olmak üzere iki ana yaklaşım bulunur.

Denetimli Öğrenme (Supervised Learning)

- **Temel Fikir:** Model, **etiketlenmiş** verilerle eğitilir. Yani, girdi verisi ile birlikte bu girdiye karşılık gelen **doğru çıktı** (etiket) modele sağlanır. Tıpkı bir öğretmenin gözetiminde öğrenmek gibidir.

- **Veri Tipi:** **Etiketli** (labeled) veri kümesi kullanılır.

- **Amaç:** Modelin, girdi ve çıktı arasındaki ilişkiyi öğrenerek **yeni girdiler için doğru çıktıyı tahmin etmesidir**.
- **Kullanım Alanları (Örnekler):**
    
    - **Sınıflandırma (Classification):** Bir veriyi önceden tanımlanmış kategorilerden birine atama.
        - Örn: Bir e-postanın **spam** ya da **spam değil** olarak sınıflandırılması.
        - Örn: Bir resimdeki nesnenin (kedi, köpek vb.) tanımlanması.
    
    - **Regresyon (Regression):** Sürekli bir çıktı değeri tahmin etme.
        - Örn: Evin büyüklüğü, konumu gibi verilere dayanarak **ev fiyatının** tahmin edilmesi.
        - Örn: Hava durumuna göre **trafik yoğunluğunun** tahmin edilmesi.


Denetimsiz Öğrenme (Unsupervised Learning)

- **Temel Fikir:** Model, **etiketlenmemiş** verilerle eğitilir. Modele yalnızca girdi verisi verilir ve doğru çıktı bilgisi sağlanmaz. Model, verilerin içindeki **gizli yapıları, örüntüleri** ve ilişkileri kendi başına keşfetmeye çalışır. .

![[Pasted image 20260102023446.png]]

### Regresyon (Regression) Lineer Regresyon

- **Amaç:** **Sürekli (Continuous)** ve **Sayısal** bir çıktı değeri tahmin etmektir.
- **Çıktı Tipi:** Sonsuz sayıda değer alabilen reel sayılar kümesi.

- **Örnekler:**
    - **Ev fiyatı tahmini:** Bir evin özelliklerine (büyüklük, oda sayısı vb.) dayanarak tam **fiyatını** (örneğin 500.000 TL) tahmin etmek.
    - **Sıcaklık tahmini:** Yarınki hava sıcaklığının **derece cinsinden** (örneğin $25.5^{\circ}C$) tahmin edilmesi.
    - **Satış geliri tahmini:** Bir reklam kampanyasından elde edilecek **gelir miktarının** tahmin edilmesi.

- Veri noktalarına en uygun geçen bir **doğru veya eğri** (en uygun çizgi - best-fit line) bulmayı amaçlar.

![linear regression best-fit line resmi](https://encrypted-tbn3.gstatic.com/licensed-image?q=tbn:ANd9GcTtDCDq1BjtErB_GvyCUYXEtbX3J88qJFxpt0YJH9YOit7IT2LTGX9MiorZ9ZI7Y7b1MNp9JstUZ_zfKlOc4pUo5Xr_sGhbdMxx9k_eg7hWx0rtLkI)


![[Pasted image 20260102212155.png]]

```python
from sklearn.linear_model import LinearRegression
import pandas as pd

data = pd.read_csv("Student_Marks.csv")


Y = data[["Marks"]]
X = data[["number_courses","time_study"]]

model = LinearRegression()

model.fit(X, Y)

# 3,6.335,32.357

model_tahmin = model.predict([[3, 6.335]]) # =  32.33884377

print("Tahmin :", model_tahmin)

print("Score :", model.score(X, Y))
```

CSV = https://www.kaggle.com/datasets/yasserh/student-marks-dataset/data

### Sınıflandırma (Classification) Karar Ağaçları - KNN 

- **Amaç:** Bir veri noktasını önceden tanımlanmış **ayrık (Discrete)** ve **Kategorik** sınıflardan birine atamaktır.
- **Çıktı Tipi:** Sınırlı ve sonlu sayıda kategori (etiket).
    -  $\text{Kedi}, \text{Köpek}, \text{Kuş}$ veya $0, 1$
- **Örnekler:**
    
    - **Spam filtresi:** Bir e-postanın **Spam** veya **Spam Değil** olarak etiketlenmesi.
    - **Hastalık teşhisi:** Hastanın semptomlarına göre bir hastalığa **sahip olup olmadığı (Evet/Hayır)** veya hangi hastalık **tipine** sahip olduğu.
    - **Görüntü tanıma:** Bir resimde **Kedi** mi yoksa **Köpek** mi olduğunu belirleme.


## NAİVE Bayes

Naive Bayes öğrenme tekniği temeli Bayes teoremine dayanır. Bayes Teoremi bir sonucun sebebini bulurken sonucun hangi olasılıkla hangi sebepten kaynaklandığını bulur.

![[Pasted image 20260102035355.png]]

![[Pasted image 20260102042845.png]]


![[Pasted image 20260102043116.png]]


![[Pasted image 20260102033834.png]]

Yukarıdaki veri setine 
X1: EĞİTİM = YÜKSEK  
X2: YAŞ = ORTA,
X3 : CİNSİYET = KADIN 
verisi eklenecektir.
Bu veri için KABUL = ?

Adım 1:
KABUL’deki Evet ve Hayır sınıflarının olasılıkları hesaplanır. 

C1 : KABUL = EVET ise P( C1 ) : 5/8 

C2 : KABUL = HAYIR : ise P( C2 ) : 3/8

Adım 2 : 

| **Nitelik (X)** | **Değer** | **Evet Sayısı** | **P(X∥C1​ (Evet Olasılığı)** | **Hayır Sayısı** | **P(X∥C2​ (Hayır Olasılığı)** |
| --------------- | --------- | --------------- | ---------------------------- | ---------------- | ----------------------------- |
| **EĞİTİM**      | YÜKSEK    | 1               | **1/5**                      | 1                | **1/3**                       |
| **YAŞ**         | ORTA      | 3               | **3/5**                      | 1                | **1/3**                       |
| **CİNSİYET**    | KADIN     | 2               | **2/5**                      | 2                | **2/3**                       |
Adım 3: P( X | C1 )**P( C1 ) ve P( X | C2)*P( C2 ) değerleri hesaplanır, en büyüğü sınıfı belirler.

P( X1 | C1 ) = P( EĞİTİM = YÜKSEK | KABUL = EVET ) : 1/5 
P( X2 | C1 ) = P( YAŞ = ORTA | KABUL = EVET ) : 3/5 
P( X3 | C1 ) = P( CİNSİYET = KADIN | KABUL = EVET ) : 2/5

P( X | C1 ) = P ( X | KABUL = EVET ) :  1 / 5 * 3/5 * 2/5  = 6 / 125
P( X | C1 ) = 6 / 125

-----------------

P( X1 | C2 ) = P( EĞİTİM = YÜKSEK | KABUL = HAYIR ) : 1/3 
P( X2 | C2 ) = P( YAŞ = ORTA | KABUL = HAYIR ) : 1/3
P( X3 | C2 ) = P( CİNSİYET = KADIN | KABUL = HAYIR ) : 2/3 

P( X | C2 ) = P( X | KABUL = HAYIR ) : 1/3 * 1/3 * 1/3 = 2/27 
P( X | C2 ) =  2/27                          * 3/8 = 0.028

P( X | C1 ) * P( C1 )  = 6 /125 * 5 / 8 = 0.03 ihtimalle evet koşulu
P( X | C2) * P( C2 )   = 2/27  *  3/8  =  0.028 ihtimalle hayır koşulu

KABUL = EVET

Sonuç : P( X | C1 )*P( C1 ) > P( X | C2)*P( C2 ) olduğu için KABUL = EVET olacaktır.

---

## KOD uygulama

- **N (0):** Normal Atım
- **S (1):** Supraventriküler Ektopik Atım
- **V (2):** Ventriküler Ektopik Atım
- **F (3):** Füzyon Atımı
- **Q (4):** Bilinmeyen Atım (Sınıflandırılamayan Atım)

data =  [ECG Heartbeat Categorization Dataset | Kaggle](https://www.kaggle.com/datasets/shayanfazeli/heartbeat?resource=download&select=mitbih_train.csv)

![[Pasted image 20260102215100.png]]

![[Pasted image 20260102215142.png]]

```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.naive_bayes import GaussianNB # CategoricalNB yerine GaussianNB önerilir
from sklearn.metrics import confusion_matrix, accuracy_score


# Dosyaları yükle
train = pd.read_csv("mitbih_train.csv", header=None)
test = pd.read_csv("mitbih_test.csv", header=None)


# Veriyi ayır

X_train = train.iloc[:, :187].values
y_train = train.iloc[:, 187].values
X_test = test.iloc[:, :187].values
y_test = test.iloc[:, 187].values

# Modeli eğit
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)
  

# Hata Matrisini Oluştur
cm = confusion_matrix(y_test, y_pred)
classes = ['N', 'S', 'V', 'F', 'Q'] # Sınıf isimleri (MIT-BIH için standart)
cm_df = pd.DataFrame(cm, index=classes, columns=classes)
  

# Çizim
plt.figure(figsize=(10,6))
sns.heatmap(cm_df, annot=True, fmt="d", cmap="YlGnBu")
plt.title("Confusion Matrix - GaussianNB")
plt.ylabel("Gerçek Sınıflar")
plt.xlabel("Tahmin Edilen Sınıflar")
plt.show()

print("Accuracy:", accuracy_score(y_test, y_pred))
```






```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

  

# 1. Veri Yükleme
veri = pd.read_csv("hayvanatbahcesi.csv", encoding='unicode_escape')

  

# 2. Tüm Veri Seti Üzerinde One-Hot Encoding Uygulama
veri_encoded = pd.get_dummies(veri.drop(["sinifi"], axis=1))

  

# 3. Giriş ve Çıkış Matrislerini Oluşturma
girisler = np.array(veri_encoded
cikis = np.array(veri["sinifi"])

  

# 4. Veriyi Bölme
X_train, X_test, y_train, y_test = train_test_split(
    girisler, cikis, test_size=0.35, random_state=109
)

  

# 5. Modeli Eğitme ve Tahmin
gnb = ComplementNB()

gnb.fit(X_train, y_train)

y_pred = gnb.predict(X_test)

y_only = gnb.predict(X_test[0:1, :])

print(" test verisi " ,X_test[0:1, :])

print(" gerçek değer:", y_test[0:1])

print("Tek bir örnek için tahmin:", y_only)

  
  

sinif_isimleri = {

    1: 'Memeli',

    2: 'Kuş',

    3: 'Sürüngen',

    4: 'Balık',

    5: 'Amfibi',

    6: 'Böcek',

    7: 'Omurgasız'

}

  

cm = confusion_matrix(y_test, y_pred)

  

# Gerçekte var olan sınıfların ID'lerini alıyoruz

sinif_idleri = np.unique(y_test)

# ID'leri kullanarak isim listesini oluşturuyoruz

classes = [sinif_isimleri.get(int(id), f'Sınıf {id}') for id in sinif_idleri]

  

cm_df = pd.DataFrame(cm, columns=classes, index=classes)

  

plt.figure(figsize=(10,6))

sns.heatmap(cm_df, annot=True, fmt="d", cmap="YlGnBu")

plt.title("Karışıklık Matrisi (ComplementNB) - Sınıf İsimleri ile")

plt.ylabel("Gerçek Sınıflar")

plt.xlabel("Tahmin Edilen Sınıflar")

plt.show()

  

# Metrikler

print("Accuracy:", accuracy_score(y_test, y_pred))

print("\nSınıflandırma Raporu:\n")
```